{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data: Student Exam Answers\n",
    "\n",
    "The data described here were used to evaluate the method of machine-scoring presented in the [Score Exams notebook](Score%20Exams.ipynb). This notebook expands upon the description of data preperation found at the prior link. \n",
    "\n",
    "More than one thousand exam answers were obtained as PDF files containing answers to thirteen free response questions, drawn from six Suffolk University Law School final exams, taught by five instructors. Each PDF corresponded to a single student exam. These PDFs were parsed to extract their answers, their contents converted into XML of the following format.\n",
    "\n",
    "```\n",
    "<EXAM>\n",
    "    <STUDENT id='ID'>00000000</STUDENT>\n",
    "    <QUESTION id='Q1'>\n",
    "        <![CDATA[\n",
    "            text of written answer to question one\n",
    "        ]]>\n",
    "    </QUESTION>\n",
    "    <QUESTION id='Q2'>\n",
    "        <![CDATA[\n",
    "            text of written answer to question two\n",
    "        ]]>\n",
    "    </QUESTION>\n",
    "    <QUESTION id='Q3'>\n",
    "        <![CDATA[\n",
    "            text of written answer to question three\n",
    "        ]]>\n",
    "    </QUESTION>\n",
    "</EXAM>\n",
    "```\n",
    "\n",
    "These translated files were reviewed by hand and reformatted as needed to correct any formatting errors. Each XML file was then read into a csv file for its associated exam as a single row with their columns corresponding to each of its elements (e.g., ID, Q1, Q2). Additionally, a column stating the number of words contained in each question was appended to the csv file (e.g., size_Q1, size_Q2). E.g., \n",
    "\n",
    "|ID|Q1|Q2|size_Q1|size_Q2|\n",
    "|--|--|--|-------|-------|\n",
    "|00001|text of 1's ans to q1|text of 1's ans to q2|6|6|\n",
    "|00002|text of 2's ans to q1|text of 2's ans to q2|6|6|\n",
    "\n",
    "Instructors also provided scores for each exam question. These were placed in a csv for each exam with the scores on each question associated to the exam ID. E.g., \n",
    "\n",
    "|ID|Q1|Q2|\n",
    "|--|--|--|\n",
    "|00001|96|93|\n",
    "|00002|83|89|\n",
    "\n",
    "In keeping with the wishes of those instructors who provided exams, only three of the exams are available here for review (i.e., [property_instructor_A](https://colarusso.github.io/free-response-scoring/exam_questions/property_instructor_A.docx), [property_instructor_B](https://colarusso.github.io/free-response-scoring/exam_questions/property_instructor_B.docx), and [crim_instructor_E](https://colarusso.github.io/free-response-scoring/exam_questions/crim_instructor_E.docx)). Another two are on file with the author and may be shared upon request and the assent of their authors. In keeping with the author's wishes, the remaining exam will not be shared. Subject to constraints imposed by the Family Educational Rights and Privacy Act (FERPA), three of the answer sets may be shared upon request. One of these is linked to an exam requiering instructor assent to be shared.\n",
    "\n",
    "Please note that the exam questiones shared at the links above are docx files and have been redacted to exclude the instructor's name and text that does not include the scored question prompts (e.g., multiple choice questions). \n",
    "\n",
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pdftotext\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Parse PDF files\n",
    "\n",
    "To facilitate retrieval of the exam answers which were stored in various folders, a list of dictionaries is defined. Each dictionary defines the folder name where csv files can be found as well as the names of its relevant columns for later consideration (i.e., their ID and those questions to be scored). To avoid sharing of this data publicly, as seen below, these files are not included in this repository and were located outside of this repository's folder (i.e., `../data/`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exams = [\n",
    "            {\"folder\":\"../data/property_instructor_A\",\"columns\":[\"ID\",\"SHORT_ANS\",\"Q1\",\"Q2\"]},\n",
    "            {\"folder\":\"../data/property_instructor_B\",\"columns\":[\"ID\",\"Q1\",\"Q2\"]},\n",
    "            {\"folder\":\"../data/environ_instructor_B\",\"columns\":[\"ID\",\"Q2\"]},\n",
    "            {\"folder\":\"../data/PR_instructor_C\",\"columns\":[\"ID\",\"Q1\",\"Q2\"]},\n",
    "            {\"folder\":\"../data/contracts_instructor_D\",\"columns\":[\"ID\",\"Q1\",\"Q2\",\"Q3\"]},\n",
    "            {\"folder\":\"../data/crim_instructor_E\",\"columns\":[\"ID\",\"Q1\",\"Q2\"]}\n",
    "          ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `data` folder contains a folder named for each exam (e.g., `property_instructor_A`). After processing of exam data, these folders contain three folders and two csv files. To begin with a file containing the exam IDs and scores (`actual.csv`) is placed in the exam folder. The individual pdf files (one for each exam are placed in the `pdfs` folder). The first round of processing places XML translations of the pdf files into the `texts` folder. A copy of these are then placed in the `texts_cleaned` folder where manual reformatting takes place. After this step, these files are compiled into the `texts.csv` file for use in the [Score Exams notebook](Score%20Exams.ipynb). This process is performed below. \n",
    "\n",
    "```\n",
    "data\n",
    "|---- property_instructor_A\n",
    "     |---- pdfs\n",
    "     |    |---- 0001_property_instructor_A.pdf \n",
    "     |    |---- 0002_property_instructor_A.pdf \n",
    "     |    |---- 0003_property_instructor_A.pdf \n",
    "     |---- texts\n",
    "     |    |---- 0001.xml \n",
    "     |    |---- 0002.xml \n",
    "     |    |---- 0003.xml \n",
    "     |---- text_cleaned\n",
    "     |    |---- 0001.xml \n",
    "     |    |---- 0002.xml \n",
    "     |    |---- 0003.xml \n",
    "     |---- actual.csv\n",
    "     |---- texts.csv     \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf (file, rules=None):\n",
    "    \n",
    "    # open the pdf file defined in `file`\n",
    "    with open(file, \"rb\") as f:\n",
    "        doc = pdftotext.PDF(f)\n",
    "        \n",
    "    # discover student ID\n",
    "    student_id = re.search(\"ID:\\s+(\\d+)\",doc[0])[1]\n",
    "    # discover exam type\n",
    "    exam_type = re.search(\"Exam Name:\\s+([^-]*)\",doc[0])[1].strip()\n",
    "    file_name = re.search(\"%s.(\\S*)\"%exam_type,doc[0])[1].strip()\n",
    "    exam_date = re.search(\"Exam Date:(.*)\",doc[0])[1].strip()\n",
    "    \n",
    "    # print details for review\n",
    "    print(\"Student ID:\",student_id)\n",
    "    print(\"Exam Type:\",exam_type)\n",
    "    print(\"File:\",file_name)\n",
    "    print(\"Pages\",len(doc))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # begin construction of XML\n",
    "    text = \"<EXAM>\\n<STUDENT id='ID'>%s</STUDENT>\\n\"%student_id\n",
    "\n",
    "    # run exam specific rules\n",
    "    if rules==\"property_instructor_B\":\n",
    "        text = text + \"<QUESTION id='Q1'>\\n<![CDATA[\\n\"\n",
    "    elif rules==\"environ_instructor_B\":\n",
    "        text = text + \"<QUESTION id='Q1'>\\n<![CDATA[\\n\"\n",
    "    elif rules==\"PR_instructor_C\":\n",
    "        text = text + \"<QUESTION id='Q1'>\\n<![CDATA[\\n\"\n",
    "    elif rules==\"crim_instructor_E\":\n",
    "        text = text + \"<QUESTION id='Q1'>\\n<![CDATA[\\n\"\n",
    "    elif rules==\"contracts_instructor_D\":\n",
    "        text = text + \"<QUESTION id='Q1'>\\n<![CDATA[\\n\"\n",
    "    \n",
    "    # for each page of the pdf\n",
    "    i = 0\n",
    "    for page in doc:\n",
    "        # for pages other than the first page\n",
    "        if i>0:\n",
    "            # parse text for construction of xml            \n",
    "            page = re.sub(\"\\s*.*xmdx\",\" \",page)\n",
    "            page = re.sub(\"\\d+\\s?of\\s?%s\"%len(doc),\" \",page)\n",
    "            page = re.sub(\"\\d+\\s+of\",\" \",page)\n",
    "            page = re.sub(\"of\\s?%s\"%len(doc),\" \",page)\n",
    "            page = re.sub(\"\\s*END OF EXAM\\s*\",\" \",page)   \n",
    "                       \n",
    "            page = re.sub(\"%s.%s\"%(exam_type,file_name),\" \",page)\n",
    "            page = re.sub(\"ID:\",\" \",page)\n",
    "            page = re.sub(exam_date,\" \",page)\n",
    "            page = re.sub(student_id,\" \",page)\n",
    "            page = re.sub(\"\\s*\\(Exam Number\\)\\s*Exam Name:\\s*Exam Date:\\s*1\\)\",\"\",page)\n",
    "\n",
    "            # run exam specific rules\n",
    "            if rules==\"property_instructor_A\":\n",
    "                page = re.sub(\"\\n\\s*((Short Answer|Part (II[^I]|2)).*)\",\"\\n\\n<QUESTION id='SHORT_ANS' tell=\\\"\\\\1\\\">\\n<![CDATA[\\n\",page, 0, re.IGNORECASE)\n",
    "                page = re.sub(\"\\n\\s*(Part (III|3).*)\",\"\\n\\n]]>\\n</QUESTION>\\n\",page, 0, re.IGNORECASE)\n",
    "                page = re.sub(\"\\n\\s*((Essay\\s)?(Question|Quesiton)\\s?1.*)\",\"\\n\\n<QUESTION id='Q1' tell=\\\"\\\\1\\\">\\n<![CDATA[\\n\",page, 0, re.IGNORECASE)\n",
    "                page = re.sub(\"\\n\\s*((Essay\\s)?(Question|Quesiton)\\s?2.*)\",\"\\n\\n]]>\\n</QUESTION>\\n<QUESTION id='Q2' tell=\\\"\\\\1\\\">\\n<![CDATA[\\n\",page, 0, re.IGNORECASE)                \n",
    "            elif rules==\"property_instructor_B\":\n",
    "                page = re.sub(\"\\n\\s*(\\(?2(\\.|\\)|:))\",\"\\n\\n]]>\\n</QUESTION>\\n<QUESTION id='Q2' tell=\\\"\\\\1\\\">\\n<![CDATA[\\n\",page, 0, re.IGNORECASE)                \n",
    "            elif rules==\"environ_instructor_B\":\n",
    "                page = re.sub(\"\\n\\s*(\\(?2(\\.|\\)|:))\",\"\\n\\n]]>\\n</QUESTION>\\n<QUESTION id='Q2' tell=\\\"\\\\1\\\">\\n<![CDATA[\\n\",page, 0, re.IGNORECASE)                \n",
    "            elif rules==\"PR_instructor_C\":\n",
    "                page = re.sub(\"\\n\\s*(\\(?2(\\.|\\)|:))\",\"\\n\\n]]>\\n</QUESTION>\\n<QUESTION id='Q2' tell=\\\"\\\\1\\\">\\n<![CDATA[\\n\",page, 0, re.IGNORECASE)                \n",
    "            elif rules==\"crim_instructor_E\":\n",
    "                page = re.sub(\"\\n\\s*((Essay|Question)?\\s?\\(?2(\\.|\\)|:|\\s))\",\"\\n\\n]]>\\n</QUESTION>\\n<QUESTION id='Q2' tell=\\\"\\\\1\\\">\\n<![CDATA[\\n\",page, 0, re.IGNORECASE)                \n",
    "            elif rules==\"contracts_instructor_D\":\n",
    "                page = re.sub(\"\\n\\s*((Essay|Question)?\\s?\\(?2(\\.|\\)|:|\\s))\",\"\\n\\n]]>\\n</QUESTION>\\n<QUESTION id='Q2' tell=\\\"\\\\1\\\">\\n<![CDATA[\\n\",page, 0, re.IGNORECASE)                \n",
    "                page = re.sub(\"\\n\\s*((Essay|Question)?\\s?\\(?3(\\.|\\)|:|\\s))\",\"\\n\\n]]>\\n</QUESTION>\\n<QUESTION id='Q3' tell=\\\"\\\\1\\\">\\n<![CDATA[\\n\",page, 0, re.IGNORECASE)                \n",
    "\n",
    "            page = re.sub(\"\\r\\n \\r\\n\",\" \",page)\n",
    "            page = re.sub(\"\\r\\n \",\"\\n\\n\",page)\n",
    "            page = re.sub(\"\\r\\n\",\" \",page)\n",
    "\n",
    "            if (page.strip() not in text):\n",
    "                text = text + \" \" + page\n",
    "                \n",
    "        i = i + 1\n",
    "\n",
    "    text = text + \"\\n\\n]]>\\n</QUESTION>\\n</EXAM>\\n\"\n",
    "    \n",
    "    text = re.sub(\" +\",\" \",text)\n",
    "    print(\"\\n\")\n",
    "        \n",
    "    return student_id, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text (text,mode=0):\n",
    "    if mode==0:\n",
    "        content = re.sub('\\n+', ' ',  text) # remove line breaks\n",
    "        content = re.sub('\\r+', ' ',  text) # remove line breaks\n",
    "        content = re.sub('\\s+', ' ',  text) # remove line breaks\n",
    "    elif mode==1:\n",
    "        content = re.sub('[^\\w]+', \" \",  text) # clean standalone number \n",
    "        content = content.lower()\n",
    "    elif mode==2:\n",
    "        content = text.lower()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as et\n",
    "def parse_XML(xml_file): \n",
    "    xtree = et.parse(xml_file)\n",
    "    xroot = xtree.getroot()\n",
    "    cols = []\n",
    "    ans = []\n",
    "    \n",
    "    for node in xroot: \n",
    "        name = node.attrib.get(\"id\")\n",
    "        ans = ans + [clean_text(node.text)]\n",
    "        cols = cols + [name]\n",
    "    \n",
    "    out_df = pd.DataFrame([ans],columns=cols)\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each exam\n",
    "for exam in exams:\n",
    "    print(\"\\n=========================================================\")\n",
    "    print(exam[\"folder\"])\n",
    "    print(\"=========================================================\\n\")\n",
    "    files = os.listdir(\"%s/pdfs\"%exam[\"folder\"])\n",
    "    \n",
    "    # for each individual student's exam\n",
    "    for file in files:\n",
    "        # read the exam and create xml\n",
    "        student_id, text = read_pdf(\"%s/pdfs/%s\"%(exam[\"folder\"],file), rules=exam[\"folder\"])\n",
    "        # write xml to file\n",
    "        text_file = open(\"%s/texts/%s.xml\"%(exam[\"folder\"],student_id), \"w\", encoding=\"utf-8\")\n",
    "        text_file.write(\"%s\" % text)\n",
    "        text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now go copy contents of `texts` into `text_cleaned` and clean the data by hand. Then run the below.\n",
    "\n",
    "This will take the cleaned xml files and compile them into the `texts.csv` file with exam ID, question text, and a count of words for each question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(row):\n",
    "    return len(row.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "write = 1\n",
    "for exam in exams:\n",
    "    print(\"\\n=========================================================\")\n",
    "    print(exam[\"folder\"])\n",
    "    print(\"=========================================================\\n\")\n",
    "    files = os.listdir(\"%s/texts_cleaned\"%exam[\"folder\"])\n",
    "    df_tmp = pd.DataFrame()\n",
    "    i = 1\n",
    "    for file in files:\n",
    "        try:\n",
    "            df_tmp = pd.concat([df_tmp,parse_XML(\"%s/texts_cleaned/%s\"%(exam[\"folder\"],file))])\n",
    "            print(\"%s)\\t%s\"%(i,file))\n",
    "        except:\n",
    "            print(\"%s)\\t%s (error)\"%(i,file))\n",
    "        i = i + 1\n",
    "        \n",
    "    df_tmp = df_tmp.reset_index(drop=True)\n",
    "    df_tmp = df_tmp[exam[\"columns\"]].dropna()\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"N:\",len(df_tmp))\n",
    "\n",
    "    for column in [x for x in exam[\"columns\"] if (x not in \"ID\")]:\n",
    "        df_tmp[\"size_%s\"%column]= df_tmp[column].apply(word_count)\n",
    "        print(\"%s Mean Words:\"%column,int(round(df_tmp[\"size_%s\"%column].mean())))\n",
    "        \n",
    "    \n",
    "    display(df_tmp)    \n",
    "    if write ==1:\n",
    "        df_tmp.to_csv(\"%s/texts.csv\"%exam[\"folder\"], index=False, encoding=\"utf-8\")            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrub exam answers of student IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import hashlib\n",
    "from random import randrange\n",
    "\n",
    "def hashme(w,seed):\n",
    "    try:\n",
    "        w = int(w)\n",
    "        w = str(w)+str(seed)\n",
    "        h = hashlib.md5(str(w).encode('utf-8'))\n",
    "        return h.hexdigest()\n",
    "    except:\n",
    "        np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for exam in exams:\n",
    "    print(\"\\n=========================================================\")\n",
    "    print(exam[\"folder\"])\n",
    "    print(\"=========================================================\\n\")\n",
    "    \n",
    "    shutil.copyfile(\"%s/texts.csv\"%exam[\"folder\"], \"%s/texts_redacted.csv\"%exam[\"folder\"])\n",
    "    shutil.copyfile(\"%s/actual.csv\"%exam[\"folder\"], \"%s/actual_redacted.csv\"%exam[\"folder\"])\n",
    "    \n",
    "    df1_tmp = pd.read_csv(\"%s/texts_redacted.csv\"%exam[\"folder\"], encoding=\"utf-8\")\n",
    "    df2_tmp = pd.read_csv(\"%s/actual_redacted.csv\"%exam[\"folder\"], encoding=\"utf-8\")\n",
    "\n",
    "    seed = randrange(1000000)\n",
    "    \n",
    "    display(df1_tmp)\n",
    "    df1_tmp[\"ID\"] = df1_tmp[\"ID\"].apply(hashme, args=(seed,))\n",
    "    display(df1_tmp)\n",
    "    display(df2_tmp)\n",
    "    df2_tmp[\"ID\"] = df2_tmp[\"ID\"].apply(hashme, args=(seed,))\n",
    "    display(df2_tmp)\n",
    "\n",
    "    df1_tmp.to_csv(\"%s/texts_redacted.csv\"%exam[\"folder\"], index=False, encoding=\"utf-8\")  \n",
    "    df2_tmp.to_csv(\"%s/actual_redacted.csv\"%exam[\"folder\"], index=False, encoding=\"utf-8\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For exams that are going to be shared, go through and read answers to make sure there is no PII in the plain text of the answers. If there is, remove from dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
